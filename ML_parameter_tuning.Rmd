---
title: "ML income prediction"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r echo=FALSE, results='hide', warning=FALSE, error=FALSE, message=FALSE}
library(rpart)
library(rpart.plot)
library(caret)
library(Metrics)

col_names <- c("age","workclass", "fnlwgt","education","education-num","marital-status","occupation","relationship","race", "sex","capital-gain","capital-loss","hours-per-week", "native-country","over50K")
data <- read.csv("/home/pekka/Downloads/adult.data", header = FALSE, col.names = col_names)
n<- nrow(data)
nvar <- length(col_names)
```

## ML Parameter tuning
I got the dataset from [UCI Machine learning repository](https://archive.ics.uci.edu/ml/datasets/adult). The dataset has `r n` observations and `r nvar` variables. The goal variable is `over50K`, a binary variable which indicates if person's yearly income is over 50K$ or not. So this is a classification problem. I tried to predict the goal variable with 4 different ML algorithms. I also tried to improve the models by tuning the hyperparametes of the models. For evaluating the performance of the models I calculated AUC scores for each.

## Decision Tree
First I used a simple decision tree. I tried to use pruning to cut back the tree, but it turns out that the fully grown tree has the lowest cross-validated error so pruning is not needed.

```{r dt1}
# Creating a train-test-split
set.seed(1234)
train <- sample(nrow(data), 0.7*nrow(data))
data.train <- data[train,]
data.test <- data[-train,]


# Creating the model
income_model <- rpart(formula = over50K ~ ., 
                      data = data.train, 
                      method = "class")

print(income_model)

# Results
rpart.plot(x = income_model)

# Generating predicted classes
class_prediction <- predict(object = income_model,  
                            newdata = data.test,   
                            type = "class")  

# Calculating confusion matrix 
confusionMatrix(data = class_prediction,       
                reference = data.test$over50K) 

# Accuracy
dt_acc <- sum(class_prediction == data.test$over50K)/nrow(data.test)



```

Next I tried creating a gini-model and information-model by changing the `type` parameter. This changes how the algorithm chooses the variable that is used to split the set of items. There seems to be no difference between the models.

I then tried to improve the original model by pruning.

```{r dt2}
# Training an gini-based model
income_model1 <- rpart(formula = over50K ~ ., 
                       data = data.train, 
                       method = "class",
                       parms = list(split = "gini"))

# Training an information-based model
income_model2 <- rpart(formula = over50K ~ ., 
                       data = data.train, 
                       method = "class",
                       parms = list(split = "information"))

# Generating predictions on the validation set using the gini model
pred1 <- predict(object = income_model1, 
                 newdata = data.test,
                 type = "class")    

# Generating predictions on the validation set using the information model
pred2 <- predict(object = income_model2, 
                 newdata = data.test,
                 type = "class")

# Comparing classification error
ce(actual = data.test$over50K, 
   predicted = pred1)
ce(actual = data.test$over50K, 
   predicted = pred2)

plotcp(income_model)

print(income_model$cptable)

# Retrieving optimal cp value based on cross-validated error
opt_index <- which.min(income_model$cptable[, "xerror"])
cp_opt <- income_model$cptable[opt_index, "CP"]

# Pruning the model (to optimized cp value)
income_model_opt <- prune(tree = income_model, 
                         cp = cp_opt)

# Plotting the optimized model
rpart.plot(x = income_model_opt, yesno = 2)

# Generating predicted classes using the model object
class_prediction <- predict(object = income_model_opt,  
                            newdata = data.test,   
                            type = "class")  

# Calculating the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = data.test$over50K) 

dt_acc <- sum(class_prediction == data.test$over50K)/nrow(data.test)


# Generating predictions on the test set
pred <- predict(object = income_model_opt,
                newdata = data.test,
                type = "prob")

# Generating predictions on the validation set using the gini model
pred1 <- predict(object = income_model1, 
                 newdata = data.test,
                 type = "prob")    

# Generating predictions on the validation set using the information model
pred2 <- predict(object = income_model2, 
                 newdata = data.test,
                 type = "prob")


# Computing the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(data.test$over50K == " >50K", 1, 0), 
    predicted = pred[,2]) 

auc(actual = ifelse(data.test$over50K == " >50K", 1, 0), 
    predicted = pred1[,2]) 

auc(actual = ifelse(data.test$over50K == " >50K", 1, 0), 
    predicted = pred2[,2]) 

dt_preds <- pred[,2]
```
## Bagging 
Next I tried bagging. Bagging (= bootstrap aggregating) is method which is designed to improve the accuracy of machine learning algorithms. Basically what bagging does is that samples are taken from the original train data(with replacement) and models are fitted to each of the samples. The output of the models are then averaged. Interestingly, in this case, the accuracy wasn't improved by bagging.

```{r bagging}
# Specifying the training configuration
ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 5,      # 5 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC

# Cross validating the income model using "treebag" method; 
# Track AUC (Area under the ROC curve)
set.seed(1234)  
str(data.train)
levels(data.train$over50K)[1] <- "under50K"
levels(data.train$over50K)[2] <- "over50K"
income_caret_model <- train(over50K ~ .,
                            data = head(data.train, 100), 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl)

print(income_caret_model)

# Print the CV AUC
income_caret_model$results[,"ROC"]

# Generate predictions on the test set
pred <- predict(object = income_caret_model, 
                newdata = data.test,
                type = "prob")

bag_preds <- pred[,2]

# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(data.test$over50K == " >50K", 1, 0), 
    predicted = pred[,"over50K"])
```
## Random Forest
Next I created a Random Forest model. I changed the `ntree` parameter from 500(=default) to 250. Then I used the `tuneRF` function to tune the `mtry` parameter. The optimal parameter value was chosen based on the OOB error and model with the optimal value was created.
```{r randomForest, message=FALSE}
library(randomForest)
set.seed(1234)  
income_model <- randomForest(formula = over50K ~ ., 
                             data = data.train)

print(income_model)

print(importance(income_model, type=2))
varImpPlot(income_model)

# Grabbing OOB error matrix
err <- income_model$err.rate

# final OOB error rate
oob_err <- err[nrow(err), "OOB"]

plot(income_model)

# Add a legend since it doesn't have one by default
legend(x = "right", 
       legend = colnames(err),
       fill = 1:ncol(err))

# Changing ntree based on the plot
income_model <- randomForest(formula = over50K ~ ., 
                             data = data.train, ntree=250)

# Generating predicted classes using the model object
class_prediction <- predict(object = income_model,   # model object 
                            newdata = data.test,  # test dataset
                            type = "class") # return classification labels

levels(data.test$over50K)[1] <- "under50K"
levels(data.test$over50K)[2] <- "over50K"
# Calculating the confusion matrix for the test set
cm <- confusionMatrix(data = class_prediction,       # predicted classes
                      reference = data.test$over50K)  # actual classes
print(cm)



# Comparing test set accuracy to OOB accuracy
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)


# Generating predictions on the test set
pred <- predict(object = income_model,
                newdata = data.test,
                type = "prob")

# Compute the AUC (`actual` must be a binary 1/0 numeric vector)
auc1 <- auc(actual = ifelse(data.test$over50K == "over50K", 1, 0), 
    predicted = pred[,"over50K"])    

# Tuning the mtry parameter
set.seed(1234)              
res <- tuneRF(x = subset(data.train, select = -over50K),
              y = data.train$over50K,
              ntreeTry = 250)

# Look at results
print(res)

# Finding the mtry value that minimizes OOB Error
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]
print(mtry_opt)

income_model <- randomForest(formula = over50K ~ ., 
                             data = data.train, ntree=250, mtry=mtry_opt)


# Generating predictions 
pred2 <- predict(object = income_model,
                newdata = data.test,
                type = "prob")


# Compute the AUC (`actual` must be a binary 1/0 numeric vector)
auc2 <- auc(actual = ifelse(data.test$over50K == "over50K", 1, 0), 
            predicted = pred2[,"over50K"])    

# Getting predictions from the model woth higher AUC
if(auc1 >= auc2){
  rf_preds <- pred[,"over50K"]
}else{
  rf_preds <- pred2[,"over50K"]
}

# Generating predicted classes using the model object
class_prediction <- predict(object = income_model,  
                            newdata = data.test,   
                            type = "class")  

rf_acc <- sum(class_prediction == data.test$over50K)/nrow(data.test)
```
## Gradient Boosting Machine
Finally, I fitted a gradient boosting machine model. I originally fitted a model with 5000 trees and used OOB and cross-validation to estimate the optimal number of trees. Then predictions with the optimal number of trees were generated.
```{r GBM, message=FALSE, warning=FALSE}
library(gbm)
# Converting "over50K" to 1, "under50K" to 0
data.train$over50K <- ifelse(data.train$over50K == "over50K", 1, 0)

# Training a 10000-tree GBM model
library(gbm)
set.seed(1234)
income_model <- gbm(formula = over50K ~ ., 
                    distribution = "bernoulli", 
                    data = data.train,
                    n.trees = 10000)


# summary() prints variable importance
summary(income_model)


# converting the test response col
data.test$over50K <- ifelse(data.test$over50K == "over50K", 1, 0)

# Generating predictions on the test set
preds1 <- predict(object = income_model, 
                  newdata = data.test,
                  n.trees = 10000)

# Generating predictions on the test set (scale to response)
preds2 <- predict(object = income_model, 
                  newdata = data.test,
                  n.trees = 10000,
                  type = "response")


auc(actual = data.test$over50K, predicted = preds1)  #default
auc(actual = data.test$over50K, predicted = preds2)  #rescaled


# Optimal ntree estimate based on OOB
ntree_opt_oob <- gbm.perf(object = income_model, 
                          method = "OOB", 
                          oobag.curve = TRUE)

# Training a CV GBM model
set.seed(1234)
income_model_cv <- gbm(formula = over50K ~ ., 
                       distribution = "bernoulli", 
                       data = data.train,
                       n.trees = 10000,
                       cv.folds = 2)

# Optimal ntree estimate based on CV
ntree_opt_cv <- gbm.perf(object = income_model_cv, 
                         method = "cv")

# Comparing the estimates                         
print(paste0("Optimal n.trees (OOB Estimate): ", ntree_opt_oob))                         
print(paste0("Optimal n.trees (CV Estimate): ", ntree_opt_cv))


# Generating predictions on the test set using ntree_opt_oob number of trees
preds1 <- predict(object = income_model, 
                  newdata = data.test,
                  n.trees = ntree_opt_oob)

# Generating predictions on the test set using ntree_opt_cv number of trees
preds2 <- predict(object = income_model_cv, 
                  newdata = data.test,
                  n.trees = ntree_opt_cv)   

# Generating the test set AUCs using the two sets of preditions & compare
auc1 <- auc(actual = data.test$over50K, predicted = preds1)  #OOB
auc2 <- auc(actual = data.test$over50K, predicted = preds2)  #CV 

# Comparing AUC 
print(paste0("Test set AUC (OOB): ", auc1))                         
print(paste0("Test set AUC (CV): ", auc2))

if(auc1 >= auc2){
  gbm_preds <- preds1
}else{
  gbm_preds <- preds2
}
```
## Results
Finally the AUC scores for each model are:
```{r results}
# Generating the test set AUCs using the two sets of predictions & compare
actual <- data.test$over50K
dt_auc <- auc(actual = actual, predicted = dt_preds)
bag_auc <- auc(actual = actual, predicted = bag_preds)
rf_auc <- auc(actual = actual, predicted = rf_preds)
gbm_auc <- auc(actual = actual, predicted = gbm_preds)

# Print results
sprintf("Decision Tree Test AUC: %.3f", dt_auc)
sprintf("Bagged Trees Test AUC: %.3f", bag_auc)
sprintf("Random Forest Test AUC: %.3f", rf_auc)
sprintf("GBM Test AUC: %.3f", gbm_auc)

auc_list <- c(dt_auc, bag_auc, rf_auc, gbm_auc)
auc_models <- c("Decision Tree", "Bagged Tree", "Random Forest", "GBM")

#print(paste("The best model is:", auc_models[which(auc_list == max(auc_list))], "With AUC score of", round(max(auc_list), 4)))
best_model <- auc_models[which(auc_list == max(auc_list))]
best_auc <- round(max(auc_list), 3)
```
So it seems that the best model for prediction is: `r best_model` with the AUC score of `r best_auc`